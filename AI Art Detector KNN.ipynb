{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier\n",
    "\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import fft\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state: int = 111\n",
    "image_height: int  = 256\n",
    "image_width: int = 256\n",
    "data_directory: str = 'data'\n",
    "\n",
    "# True balances dataset to 10k of each class, false uses 20k generated images and 10k real images\n",
    "balance_dataset_TF: bool = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir):\n",
    "    '''\n",
    "    Loads and preprocesses image dataset. \n",
    "    Adjust preprocessing steps here and the preprocess_image function.\n",
    "    Requires only the image path. \n",
    "\n",
    "    Returns two arrays of the images and the labels. \n",
    "    '''\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(dataset_dir))\n",
    "    \n",
    "    for ii, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            image = cv2.imread(image_path)  # Load image\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Make grayscale\n",
    "            image = cv2.resize(image, (image_height, image_width))  # Resize\n",
    "            image = exposure.equalize_hist(image) # Histogram equalization\n",
    "            images.append(image)\n",
    "            labels.append(ii)  # Assign a label to the image based on the class index\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def extract_hog_features(images):\n",
    "    '''\n",
    "    Runs the hog() method from Scikit-Image on the dataset.\n",
    "    Assumes images will be grayscale as channel_axis = None by default.'\n",
    "\n",
    "    Returns an array of image features and an array of the images.\n",
    "    '''\n",
    "    hog_features = []\n",
    "    hog_images = []\n",
    "    for img in images:\n",
    "        feature_vector, image = hog(img, orientations = 8, pixels_per_cell = (8, 8),\n",
    "                 cells_per_block = (1, 1), visualize = True, feature_vector = True)\n",
    "        hog_features.append(feature_vector)\n",
    "        hog_images.append(image)\n",
    "    return np.array(hog_features), np.array(hog_images)\n",
    "\n",
    "def extract_glcm_features(images):\n",
    "    '''\n",
    "    Runs the graycomatrix() and graycoprops() methods from Scikit-Image on the dataset.\n",
    "    Assumes images will be grayscale. \n",
    "\n",
    "    Returns an array of four image features derived from the GLCM Matrix of each image.\n",
    "    '''\n",
    "    glcm_features = []\n",
    "    for img in images:\n",
    "        img = np.uint8(img * 255) # graycomatrix only accepts ints \n",
    "        # Calculate the GLCM Matrix\n",
    "        glcm_matrix = graycomatrix(img, distances = [5], angles = [0], levels = 256, symmetric = True, normed = True)\n",
    "\n",
    "        # Derive the Properties of the Matrix\n",
    "        contrast = graycoprops(glcm_matrix, 'contrast')[0, 0]\n",
    "        correlation = graycoprops(glcm_matrix, 'correlation')[0, 0]\n",
    "        energy = graycoprops(glcm_matrix, 'energy')[0, 0]\n",
    "        homogeneity = graycoprops(glcm_matrix, 'homogeneity')[0, 0]\n",
    "\n",
    "        glcm_features.append([contrast, correlation, energy, homogeneity])\n",
    "    \n",
    "    return np.array(glcm_features)\n",
    "\n",
    "def extract_spectral_features(images):\n",
    "    '''\n",
    "    Replicates the feature extraction process from Bammey (2024).\n",
    "    Assumes images will be grayscale.\n",
    "\n",
    "    The process finds the edges of an image through Cross Difference calculations.\n",
    "    Then a Fast Fourier Transform is applied to process the signal in the frequency domain.\n",
    "    Finally, the paper proposes using Peak Magnitude Extraction to identify important features.\n",
    "\n",
    "    Returns an Peak Magnitudes of images.\n",
    "    '''\n",
    "    spectral_features = []\n",
    "    cross_difference_filter = np.array([[-1, -1, -1],\n",
    "                                        [-1, 9, -1],\n",
    "                                        [-1, -1, -1]])\n",
    "\n",
    "    for ii in range(len(images)):\n",
    "        image = images[ii]\n",
    "        cross_difference = convolve(image, cross_difference_filter) # Cross Difference using defined filter\n",
    "        fft_image = fft.fft2(cross_difference) # FFT on the Cross Difference\n",
    "        peak_magnitude_spectrum = np.abs(fft_image) # Take Absolute Value to find largest peaks\n",
    "        peaks, properties = find_peaks(peak_magnitude_spectrum.flatten(), height = 0)\n",
    "        peak_magnitudes = peak_magnitude_spectrum.flatten()[peaks]\n",
    "\n",
    "        spectral_features.append(peak_magnitudes)\n",
    "\n",
    "    # Number of peaks will vary, so need to pad smaller feature sets\n",
    "    max_length = max(len(feature) for feature in spectral_features)\n",
    "\n",
    "    padded_features = [np.pad(feature, (0, max_length - len(feature)), mode = 'constant') for feature in spectral_features]\n",
    "\n",
    "    return np.array(padded_features)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    '''\n",
    "    Loads and preprocesses a single image for classification.\n",
    "    Requires only the image path.\n",
    "    '''\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Make grayscale\n",
    "    image = cv2.resize(image, (image_height, image_width)) # Resize\n",
    "    image = exposure.equalize_hist(image) # Histogram equalization\n",
    "    features = extract_spectral_features([image])\n",
    "    return features\n",
    "\n",
    "def return_image_class(class_number):\n",
    "    if class_number == 0:\n",
    "        return \"AI Generated\"\n",
    "    else:\n",
    "        return \"Real\"\n",
    "\n",
    "def balance_dataset(balance_dataset_TF, images, labels):\n",
    "    ''' \n",
    "    Optional: Randomly under sample majority class (AI-generated) for more balaanced dataset\n",
    "    Uses a random state from the global variable random_state.\n",
    "    Change balance_dataset_TF to True for balanced or False for imbalanced. \n",
    "\n",
    "    Returns either a balanced 50/50 dataset or the original 2/3 fake, 1/3 real dataset as arrays. \n",
    "    '''\n",
    "    if balance_dataset_TF == True:\n",
    "        indicies_of_majority_class = [x for x, label in enumerate(labels) if label == 0]\n",
    "        random.seed(random_state) # Set seed for reproducibility\n",
    "        random_indicies = random.sample(indicies_of_majority_class, 10000) # Randomly select 10000 fake images\n",
    "        random_undersampled_images = [images[index] for index in random_indicies]\n",
    "        random_undersampled_labels = [labels[index] for index in random_indicies]\n",
    "        \n",
    "        #Add back real images to randomly selected generated images\n",
    "        balanced_image_data = random_undersampled_images + [images[index] for index, label in enumerate(labels) if label == 1]\n",
    "        balanced_label_data = random_undersampled_labels + [label for label in labels if label == 1]\n",
    "        return np.array(balanced_image_data), np.array(balanced_label_data)\n",
    "    \n",
    "    elif balance_dataset_TF == False:\n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "def create_roc_auc_graph(y_test, y_pred_list, names_list):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    color_list = ['blue', '#1b9e77', '#7570b3']\n",
    "    for ii in range(len(y_pred_list)):\n",
    "        # Calculate scores using SKLearn metrics\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_list[ii])\n",
    "        roc_auc_score_store = roc_auc_score(y_test, y_pred_list[ii])\n",
    "\n",
    "        # Plot Graph with Matplotlib\n",
    "        plt.plot(fpr, tpr, color = color_list[ii], lw = 2, label = f'ROC Curve for {names_list[ii]} (Area = {roc_auc_score_store:.2f})')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], color = 'orange', linestyle = '--', label = 'Random Guess')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_dataset(data_directory)\n",
    "images, labels = balance_dataset(balance_dataset_TF, images, labels)\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Generated Images: {labels[labels == 0].shape[0]}')\n",
    "print(f'Number of Real Images: {labels[labels == 1].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images and extract features (Using HOG)\n",
    "hog_features, hog_images = extract_hog_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images and extract features (Using GLCM)\n",
    "glcm_features = extract_glcm_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images and extract features (Using Spectral Analysis)\n",
    "# **Best Performing Festure Set**\n",
    "spectral_features = extract_spectral_features(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_image_to_display = 17000\n",
    "class_for_display = return_image_class(labels[which_image_to_display])\n",
    "untransformed_image = images[which_image_to_display]\n",
    "hog_image = exposure.rescale_intensity(hog_images[which_image_to_display], in_range=(0, 10))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex = True, sharey = True)\n",
    "ax1.axis('off')\n",
    "ax1.imshow(untransformed_image, cmap=plt.cm.gray)\n",
    "ax1.set_title(f'Original Image ({class_for_display})')\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(hog_image, cmap=plt.cm.gray)\n",
    "ax2.set_title('HOG Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Exmaple\n",
    "which_image_to_display = 17000\n",
    "class_for_display = return_image_class(labels[which_image_to_display])\n",
    "untransformed_image = images[which_image_to_display]\n",
    "\n",
    "cross_difference_filter = np.array([[-1, -1, -1],\n",
    "                                    [-1, 9, -1],\n",
    "                                    [-1, -1, -1]])\n",
    "\n",
    "cross_difference = convolve(untransformed_image, cross_difference_filter)\n",
    "fft_image = fft.fft2(cross_difference) \n",
    "peak_magnitude_spectrum = np.abs(fft_image)\n",
    "peaks, properties = find_peaks(peak_magnitude_spectrum.flatten(), height = 0)\n",
    "peak_magnitudes = peak_magnitude_spectrum.flatten()[peaks]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 12), sharex = False, sharey = False)\n",
    "ax1.axis('off')\n",
    "ax1.imshow(untransformed_image, cmap=plt.cm.gray)\n",
    "ax1.set_title(f'Original Image ({class_for_display})')\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(cross_difference, cmap=plt.cm.gray)\n",
    "ax2.set_title('Cross Difference')\n",
    "\n",
    "ax3.axis('off')\n",
    "ax3.imshow(peak_magnitude_spectrum, cmap='gist_stern')\n",
    "ax3.set_title('FFT')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for HOG Features\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(hog_features, labels, test_size = 0.3, random_state = random_state)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes = np.unique(y_train), y = y_train)\n",
    "print(f'Class Weights: {class_weights}')\n",
    "\n",
    "# Train the KNN model\n",
    "knn_model = KNeighborsClassifier(weights='distance')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_hog = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLCM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for GLCM Features\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(glcm_features, labels, test_size = 0.3, random_state = random_state)\n",
    "\n",
    "# Train the KNN model\n",
    "knn_model = KNeighborsClassifier(weights='distance')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_glcm = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Analysis Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for Spectral Analysis\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectral_features, labels, test_size = 0.3, random_state = random_state)\n",
    "\n",
    "pca = PCA(n_components=128)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train the SVM model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_sa = knn_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_hog = accuracy_score(y_test, y_pred_hog)\n",
    "precision_hog = precision_score(y_test, y_pred_hog)\n",
    "recall_hog = recall_score(y_test, y_pred_hog)\n",
    "f1_hog = f1_score(y_test, y_pred_hog)\n",
    "\n",
    "accuracy_glcm = accuracy_score(y_test, y_pred_glcm)\n",
    "precision_glcm = precision_score(y_test, y_pred_glcm)\n",
    "recall_glcm = recall_score(y_test, y_pred_glcm)\n",
    "f1_glcm = f1_score(y_test, y_pred_glcm)\n",
    "\n",
    "accuracy_sa = accuracy_score(y_test, y_pred_sa)\n",
    "precision_sa = precision_score(y_test, y_pred_sa)\n",
    "recall_sa = recall_score(y_test, y_pred_sa)\n",
    "f1_sa = f1_score(y_test, y_pred_sa)\n",
    "\n",
    "print(f\"HOG - Accuracy: {accuracy_hog:.2f}, Precision: {precision_hog:.2f}, Recall: {recall_hog:.2f}, F1 Score: {f1_hog:.2f} \\n\")\n",
    "print(f\"GLCM - Accuracy: {accuracy_glcm:.2f}, Precision: {precision_glcm:.2f}, Recall: {recall_glcm:.2f}, F1 Score: {f1_glcm:.2f} \\n\")\n",
    "print(f\"Spectral Analysis - Accuracy: {accuracy_sa:.2f}, Precision: {precision_sa:.2f}, Recall: {recall_sa:.2f}, F1 Score: {f1_sa:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_hog, labels = knn_model.classes_)\n",
    "\n",
    "display_matrix = ConfusionMatrixDisplay(confusion_matrix = conf_matrix, display_labels = knn_model.classes_).plot(cmap = 'PuBu')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_sa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [y_pred_hog, y_pred_glcm, y_pred_sa]\n",
    "create_roc_auc_graph(y_test, y_pred_list, ['HOG', 'GLCM', 'Spectral Analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_neighbors\": [1, 2, 3, 4, 5, 10, 25, 50], #Number of neighbors, 5 is default\n",
    "    \"weights\": [\"distance\", \"uniform\"], #Uniform is default\n",
    "}\n",
    "\n",
    "# setup the grid search\n",
    "grid_search = GridSearchCV(knn_model,\n",
    "                           param_grid=params,\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a nice table for the Grid Search\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df = results_df.sort_values(by=[\"rank_test_score\"])\n",
    "results_df = results_df.set_index(results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))).rename_axis(\"kernel\")\n",
    "results_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectral_features, labels, test_size = 0.3, random_state = random_state)\n",
    "\n",
    "pca = PCA(n_components=128)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 50, weights = 'distance')\n",
    "knn_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluate the model performance\n",
    "y_pred = knn_model.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels = knn_model.classes_)\n",
    "\n",
    "display_matrix = ConfusionMatrixDisplay(confusion_matrix = conf_matrix, display_labels = knn_model.classes_).plot(cmap = 'PuBu')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "Saving the Spectral Analysis Model as it was the best performing for KNN.\n",
    "\n",
    "Note that the PCA fitted model also would need to be saved off and loaded for another application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "model_to_save = dump(knn_model, filename = 'models/ModelKNN.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading Model and Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_path = \"test-real.jpg\"\n",
    "\n",
    "# Preprocess the new image using spectral analysis\n",
    "new_image_features = preprocess_image(new_image_path)\n",
    "\n",
    "#Pad to the max length of the feature set for PCA\n",
    "padded_features = np.pad(new_image_features, (0, 23955 - len(new_image_features[0])), mode='constant')\n",
    "\n",
    "new_features_pca = pca.transform(padded_features)\n",
    "\n",
    "# Make a prediction using the reloaded SVM model\n",
    "reloaded_model = load('models/ModelKNN.sav')\n",
    "predicted_class = return_image_class(reloaded_model.predict(new_features_pca)[0])\n",
    "\n",
    "print(f'Predicted Class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model can be loaded into other applications so long as the preprocessing steps are the same as in this pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
